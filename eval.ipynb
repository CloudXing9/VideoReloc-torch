{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch implementation of Video Re-loc\")\n",
    "\n",
    "parser.add_argument('--inter_op_parallelism_threads', default=0, type=int,\n",
    "                    help='number of threads')\n",
    "parser.add_argument('--intra_op_parallelism_threads', default=0, type=int,\n",
    "                    help='number of threads')                    \n",
    "parser.add_argument('--max_length', default=300, type=int,\n",
    "                    help='max length')\n",
    "parser.add_argument('--feat_dim', default=500, type=int,\n",
    "                    help='feature dim')\n",
    "parser.add_argument('--keep_prob', default=0.6, type=float,\n",
    "                    help='keep prob')                    \n",
    "parser.add_argument('--mem_dim', default=128, type=int,\n",
    "                    help='hidden state dim')\n",
    "parser.add_argument('--att_dim', default=128, type=int,\n",
    "                    help='attention dim')    \n",
    "parser.add_argument('--job_dir', default='saving', type=str,\n",
    "                    help='job_dir')    \n",
    "parser.add_argument('--data_dir', default='data', type=str,\n",
    "                    help='dir')               \n",
    "parser.add_argument('--num_gpus', default=0, type=int,\n",
    "                    help='number of gpus')\n",
    "parser.add_argument('--bucket_span', default=30, type=int,\n",
    "                    help='bucket_span')                    \n",
    "parser.add_argument('--batch_size', default=128, type=int,\n",
    "                    help='batch_size')\n",
    "parser.add_argument('--max_steps', default=1000, type=int,\n",
    "                    help='max_steps')\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float,\n",
    "                    help='weight decay')                    \n",
    "parser.add_argument('--learning_rate', default=0.001, type=float,\n",
    "                    help='learning_rate')\n",
    "parser.add_argument('--max_gradient_norm', default=5.0, type=float,\n",
    "                    help='max_gradient_norm')    \n",
    "parser.add_argument('--save_summary_steps', default=10, type=int,\n",
    "                    help='save_summary_steps')  \n",
    "parser.add_argument('--save_checkpoint_steps', default=100, type=int,\n",
    "                    help='save_checkpoint_steps')  \n",
    "global args\n",
    "args = parser.parse_args()\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, args, subset):\n",
    "        self.is_training = subset == 'train'\n",
    "        with open(os.path.join('data', subset + '.json'), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.videos = [[] for _ in range(4)]\n",
    "        for i in data:\n",
    "            self.videos[0].append(i['id'])\n",
    "            self.videos[1].append(i['groundtruth'])\n",
    "            self.videos[2].append(i['label'])\n",
    "            self.videos[3].append(i['location'])\n",
    "        for i in range(1,4):\n",
    "            self.videos[i] = torch.tensor(self.videos[i])\n",
    "\n",
    "        self.dataset = self.videos\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def map_fn(func,inputs,extra=None):\n",
    "            outputs = []\n",
    "            length = len(inputs[0])\n",
    "            for i in range(length):\n",
    "                inp_func = []\n",
    "                for x in inputs:\n",
    "                    inp_func.append(x[i])\n",
    "                if extra:\n",
    "                    inp_func.extend(extra)\n",
    "                outputs.append(func(inp_func))\n",
    "            return outputs\n",
    "\n",
    "        def map_fn2(func,inputs,extra=None):\n",
    "            outputs = []\n",
    "            length = len(inputs[0])\n",
    "            for x in inputs:\n",
    "                inp_func = []\n",
    "                for i in range(length):\n",
    "                    inp_func.append(x[i])\n",
    "                if extra:\n",
    "                    inp_func.extend(extra)\n",
    "                outputs.append(func(inp_func))\n",
    "            return outputs\n",
    "\n",
    "        def sample(inputs):\n",
    "            query_id, query_gt, query_label, query_loc, all_ids, all_gts, all_labels, all_locs, is_training = inputs\n",
    "\n",
    "            same =(all_labels == query_label)\n",
    "            longer = query_gt[1] - query_gt[0] < all_locs[:,1] - all_locs[:,0]\n",
    "            same = torch.logical_and(same,longer)\n",
    "            same = torch.where(same)[0]\n",
    "            num = same.shape[0]\n",
    "            if num == 0:\n",
    "                return\n",
    "            idx = np.random.choice(same)\n",
    "            chosen_id = all_ids[idx]\n",
    "            chosen_gt = all_gts[idx]\n",
    "            chosen_loc = all_locs[idx]\n",
    "            if is_training:\n",
    "                off_st = torch.randint(chosen_gt[0] + 1,[])\n",
    "                maxval = chosen_loc[1] - chosen_loc[0] - chosen_gt[1] + 1\n",
    "                off_en = torch.randint(maxval,[])\n",
    "                use_off = torch.rand([])\n",
    "                off_st = off_st if use_off < 0.9 else torch.tensor(0)\n",
    "                off_en = off_en if use_off < 0.9 else torch.tensor(0)\n",
    "                off_gt = torch.stack([-off_st,-off_st])\n",
    "                off_loc = torch.stack([off_st, -off_en])\n",
    "                chosen_gt += off_gt\n",
    "                chosen_loc += off_loc\n",
    "            return query_id, query_gt + query_loc[0], chosen_id, chosen_gt, chosen_loc\n",
    "\n",
    "\n",
    "        def get_data(inputs):\n",
    "            v1, t1, v2, t2, l2,data_dir = inputs\n",
    "            \"\"\"Read the video features.\"\"\"\n",
    "            feat1 = np.load('%s/feat/v_%s.npy' % (data_dir, v1))\n",
    "            feat2 = np.load('%s/feat/v_%s.npy' % (data_dir, v2))\n",
    "            len1 = t1[1] - t1[0]\n",
    "            len2 = l2[1] - l2[0]\n",
    "            ret1 = feat1[t1[0]:t1[1]]\n",
    "            ret2 = feat2[l2[0]:l2[1]] \n",
    "            assert len1 == ret1.shape[0]\n",
    "            assert len2 == ret2.shape[0]\n",
    "            assert (t2 >= 0).all() and (t2 <= len2).all()\n",
    "            return ret1, len1, ret2, len2, t2\n",
    "\n",
    "        def get_bucket_span(ret):\n",
    "            gt = [[]for i in range(11)]\n",
    "            for i in ret:\n",
    "                l = i[-1][1]-i[-1][0]\n",
    "                l = l // args.bucket_span\n",
    "                gt[l].append(i)\n",
    "            return gt\n",
    "        \n",
    "        def random_choice(gt):\n",
    "            l = []\n",
    "            for x in gt:\n",
    "                l.append(len(x))\n",
    "            total=np.sum(l)\n",
    "            for i in range(len(l)):\n",
    "                l[i]=l[i]/total\n",
    "            choice_bucket = np.random.choice(gt,p=l)\n",
    "            print(l)\n",
    "            choice_batch = np.random.choice(len(choice_bucket),replace=True,size=args.batch_size)\n",
    "            ret = []\n",
    "            for index in choice_batch:\n",
    "                ret.append(choice_bucket[index])\n",
    "            return ret\n",
    "        if self.is_training:\n",
    "            ret = map_fn(sample,self.dataset,[*self.videos,self.is_training])\n",
    "            gt = get_bucket_span(ret)\n",
    "            ret = random_choice(gt)\n",
    "            ret = map_fn2(get_data,ret,[args.data_dir])\n",
    "            return ret\n",
    "        else:\n",
    "            ret = map_fn(sample,self.dataset,[*self.videos,self.is_training])\n",
    "            ret = map_fn2(get_data,ret,[args.data_dir])\n",
    "            return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return args.max_steps if self.is_training else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(args,subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(args,subset=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='test'\n",
    "is_training = subset == 'train'\n",
    "with open(os.path.join('data', subset + '.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "videos = [[] for _ in range(4)]\n",
    "for i in data:\n",
    "    videos[0].append(i['id'])\n",
    "    videos[1].append(i['groundtruth'])\n",
    "    videos[2].append(i['label'])\n",
    "    videos[3].append(i['location'])\n",
    "for i in range(1,4):\n",
    "    videos[i] = torch.tensor(videos[i])\n",
    "\n",
    "dataset = videos\n",
    "\n",
    "def map_fn(func,inputs,extra=None):\n",
    "    outputs = []\n",
    "    length = len(inputs[0])\n",
    "    for i in range(length):\n",
    "        inp_func = []\n",
    "        for x in inputs:\n",
    "            inp_func.append(x[i])\n",
    "        if extra:\n",
    "            inp_func.extend(extra)\n",
    "        outputs.append(func(inp_func))\n",
    "    return outputs\n",
    "\n",
    "def map_fn2(func,inputs,extra=None):\n",
    "    outputs = []\n",
    "    length = len(inputs[0])\n",
    "    for x in inputs:\n",
    "        inp_func = []\n",
    "        for i in range(length):\n",
    "            inp_func.append(x[i])\n",
    "        if extra:\n",
    "            inp_func.extend(extra)\n",
    "        outputs.append(func(inp_func))\n",
    "    return outputs\n",
    "\n",
    "def sample(inputs):\n",
    "    query_id, query_gt, query_label, query_loc, all_ids, all_gts, all_labels, all_locs, is_training = inputs\n",
    "\n",
    "    same =(all_labels == query_label)\n",
    "    longer = query_gt[1] - query_gt[0] < all_locs[:,1] - all_locs[:,0]\n",
    "    same = torch.logical_and(same,longer)\n",
    "    same = torch.where(same)[0]\n",
    "    num = same.shape[0]\n",
    "    if num == 0:\n",
    "        return\n",
    "    np.random.seed(6)\n",
    "    idx = np.random.choice(same)\n",
    "    np.random.seed()\n",
    "    chosen_id = all_ids[idx]\n",
    "    chosen_gt = all_gts[idx]\n",
    "    chosen_loc = all_locs[idx]\n",
    "    if is_training:\n",
    "        off_st = torch.randint(chosen_gt[0] + 1,[])\n",
    "        maxval = chosen_loc[1] - chosen_loc[0] - chosen_gt[1] + 1\n",
    "        off_en = torch.randint(maxval,[])\n",
    "        use_off = torch.rand([])\n",
    "        off_st = off_st if use_off < 0.9 else torch.tensor(0)\n",
    "        off_en = off_en if use_off < 0.9 else torch.tensor(0)\n",
    "        off_gt = torch.stack([-off_st,-off_st])\n",
    "        off_loc = torch.stack([off_st, -off_en])\n",
    "        chosen_gt += off_gt\n",
    "        chosen_loc += off_loc\n",
    "    return query_id, query_gt + query_loc[0], chosen_id, chosen_gt, chosen_loc\n",
    "\n",
    "\n",
    "def get_data(inputs):\n",
    "    v1, t1, v2, t2, l2,data_dir = inputs\n",
    "    \"\"\"Read the video features.\"\"\"\n",
    "    feat1 = np.load('%s/feat/v_%s.npy' % (data_dir, v1))\n",
    "    feat2 = np.load('%s/feat/v_%s.npy' % (data_dir, v2))\n",
    "    len1 = t1[1] - t1[0]\n",
    "    len2 = l2[1] - l2[0]\n",
    "    ret1 = feat1[t1[0]:t1[1]]\n",
    "    ret2 = feat2[l2[0]:l2[1]] \n",
    "    assert len1 == ret1.shape[0]\n",
    "    assert len2 == ret2.shape[0]\n",
    "    assert (t2 >= 0).all() and (t2 <= len2).all()\n",
    "    return ret1, len1, ret2, len2, t2\n",
    "\n",
    "ret = map_fn(sample,dataset,[*videos,is_training])\n",
    "ret = map_fn2(get_data,ret,[args.data_dir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    query = [torch.Tensor(item[0]) for item in batch]\n",
    "    query = pad_sequence(query,batch_first=True)\n",
    "    len_q = [item[1] for item in batch]\n",
    "    ref = [torch.Tensor(item[2]) for item in batch]\n",
    "    ref = pad_sequence(ref,batch_first=True)\n",
    "    len_r=[item[3] for item in batch]\n",
    "    label =[torch.tensor(item[-1]) for item in batch]\n",
    "    label = torch.stack(label)\n",
    "    return query,len_q,ref,len_r,label\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "data = DataLoader(ret,batch_size=128,shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 300, 500])\n",
      "torch.Size([128, 300, 500])\n",
      "torch.Size([128, 184, 500])\n",
      "torch.Size([128, 300, 500])\n",
      "torch.Size([128, 209, 500])\n",
      "torch.Size([128, 300, 500])\n",
      "torch.Size([128, 209, 500])\n",
      "torch.Size([82, 209, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-289-0d1092cc33e7>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label =[torch.tensor(item[-1]) for item in batch]\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(data):\n",
    "    print(item[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_fn(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(model_fn, self).__init__()\n",
    "        #query,a len_q, ref, len_r = features\n",
    "        self.batch_size = args.batch_size\n",
    "        self.features = args.feat_dim\n",
    "        self.seq_length = args.max_length\n",
    "        self.mem_dim = args.mem_dim\n",
    "        self.att_dim = args.att_dim\n",
    "        self.keep_prob = args.keep_prob\n",
    "        \n",
    "        self.cell = nn.LSTMCell(self.features,self.mem_dim)\n",
    "        self.dropout = nn.Dropout(self.keep_prob)\n",
    "        self.fwd = nn.LSTMCell(self.mem_dim,self.att_dim)\n",
    "        self.bwd = nn.LSTMCell(self.mem_dim,self.att_dim)\n",
    "        self.Match1=MatchCellWrapper(self.fwd, self.mem_dim,self.att_dim)\n",
    "        self.Match2=MatchCellWrapper(self.bwd, self.mem_dim,self.att_dim)\n",
    "        self.pointer = nn.LSTMCell(self.att_dim*2,self.att_dim)\n",
    "        self.fc = nn.Linear(self.att_dim,4)\n",
    "        self.maxlen=300\n",
    "        \n",
    "    def flipBatch(self,data, lengths):\n",
    "        assert data.shape[0] == len(lengths), \"Dimension Mismatch!\"\n",
    "        ret = data.clone()\n",
    "        for i in range(data.shape[0]):\n",
    "            ret[i,:lengths[i]] = data[i,:lengths[i]].flip(dims=[0])\n",
    "        return ret    \n",
    "    \n",
    "    def dynamic_rnn(self, model, inputs, seq_len):\n",
    "        max_time = inputs.shape[1]\n",
    "        batch = inputs.shape[0]\n",
    "        hx = torch.zeros(batch, model.hidden_size)\n",
    "        cx = torch.zeros(batch, model.hidden_size)\n",
    "        none = 0\n",
    "        for i in range(max_time):\n",
    "            hx,cx = model(inputs[:,i,:],(hx,cx))\n",
    "            for k in range(batch):\n",
    "                hx[k][int(seq_len[k]):]=0\n",
    "                cx[k][int(seq_len[k]):]=0\n",
    "            if(none == 0):\n",
    "                ret = torch.unsqueeze(hx,axis=1)\n",
    "                none = 1\n",
    "            else:\n",
    "                ret = torch.cat((ret,torch.unsqueeze(hx,axis=1)),axis=1)\n",
    "        return ret\n",
    "    \n",
    "    def dynamic_rnn_match(self, model,inputs1, inputs2, seq_len):\n",
    "        max_time = inputs2.shape[1]\n",
    "        batch = inputs2.shape[0]\n",
    "        hx = torch.zeros(batch, model._attn_vec_size)\n",
    "        cx = torch.zeros(batch, model._attn_vec_size)\n",
    "        none = 0\n",
    "        for i in range(max_time):\n",
    "            hx,cx = model(inputs1, inputs2[:,i,:],(hx,cx),seq_len)\n",
    "            for k in range(batch):\n",
    "                hx[k][int(seq_len[k]):]=0\n",
    "                cx[k][int(seq_len[k]):]=0\n",
    "            if(none == 0):\n",
    "                ret = torch.unsqueeze(hx,axis=1)\n",
    "                none = 1\n",
    "            else:\n",
    "                ret = torch.cat((ret,torch.unsqueeze(hx,axis=1)),axis=1)\n",
    "        return ret    \n",
    "    \n",
    "    def forward(self,features,labels):\n",
    "        query,len_q,ref,len_r = features\n",
    "        query = torch.autograd.Variable(query)\n",
    "        ref = torch.autograd.Variable(ref)\n",
    "        out1 = self.dynamic_rnn(self.cell, query, len_q)\n",
    "        out2 = self.dynamic_rnn(self.cell, ref, len_r)\n",
    "        out1 = self.dropout(out1)\n",
    "        out2 = self.dropout(out2)\n",
    "        \n",
    "        forward_out = self.dynamic_rnn_match(self.Match1,out1,out2,len_q)\n",
    "        out2_reverse = self.flipBatch(out2,len_r)\n",
    "        backward_out = self.dynamic_rnn_match(self.Match2,out1,out2_reverse,len_q)\n",
    "        backward_out = self.flipBatch(backward_out,len_r)\n",
    "        h = torch.cat([forward_out,backward_out],axis=2)\n",
    "        h = self.dropout(h)\n",
    "        self.maxlen = h.shape[1]\n",
    "        point_out = self.dynamic_rnn(self.pointer,h,len_r)\n",
    "        logits = self.fc(point_out)\n",
    "         \n",
    "        return logits\n",
    "    \n",
    "    def predict(self,logits,length):#len_r\n",
    "        \n",
    "        def map_fn(func,inputs):\n",
    "            outputs = []\n",
    "            length = inputs[0].shape[0]\n",
    "            for i in range(length):\n",
    "                inp_func = []\n",
    "                for x in inputs:\n",
    "                    inp_func.append(x[i])\n",
    "                outputs.append(func(inp_func))\n",
    "            return torch.stack(outputs)  \n",
    "        \n",
    "        def map_body(x):\n",
    "            logits = x[0]\n",
    "            length = x[1]\n",
    "            logits = logits[:length]\n",
    "            prob = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "            prob = prob.T\n",
    "            \n",
    "            initial_it = 0\n",
    "            initial_idx_ta = []\n",
    "            initial_val_ta = []\n",
    "            def cond(inp):\n",
    "                it = inp[0]\n",
    "                return it < min(length,64)\n",
    "\n",
    "            def while_body(x):\n",
    "                it, idx_ta, val_ta=x\n",
    "                if it == 0:\n",
    "                        total = prob[:2].sum(0)\n",
    "                else:\n",
    "                        total = prob[0, :-it] + prob[1, it:]\n",
    "\n",
    "                def get_inside():\n",
    "                    score = prob[2, None, :].repeat([it, 1])\n",
    "                    score = self.flipBatch(score, torch.zeros([it]).int() + length)\n",
    "                    score = self.flipBatch(score, length - torch.range(0,it-1).int())\n",
    "                    score = score[:, :-it]\n",
    "                    score = score.mean(0)\n",
    "                    return score\n",
    "                if it ==0:\n",
    "                    ave = prob[2]\n",
    "                else:\n",
    "                    ave = get_inside()\n",
    "\n",
    "                total += ave\n",
    "                idx = torch.argmax(total)\n",
    "                idx_ta.append(idx)\n",
    "                val_ta.append(total[idx])\n",
    "                it += 1\n",
    "                return it, idx_ta, val_ta\n",
    "\n",
    "            res = [initial_it, initial_idx_ta, initial_val_ta]\n",
    "            while(cond(res)):\n",
    "                res = while_body(res)\n",
    "            final_idx = torch.stack(res[1])\n",
    "            final_val = torch.stack(res[2])\n",
    "            idx = torch.argmax(final_val)\n",
    "            pred = torch.stack([final_idx[idx], final_idx[idx] + idx + 1])\n",
    "            return pred\n",
    "        \n",
    "        return map_fn(map_body,[logits,length])\n",
    "\n",
    "    def get_loss(self,logits,all_labels):\n",
    "        #loss\n",
    "        def sequence_mask(lengths, maxlen=None, dtype=torch.bool):\n",
    "            if maxlen is None:\n",
    "                 maxlen = lengths.max()\n",
    "            row_vector = torch.arange(0, maxlen, 1)\n",
    "            matrix = torch.unsqueeze(lengths, dim=-1)\n",
    "            mask = row_vector < matrix\n",
    "            mask.type(dtype)\n",
    "            return mask\n",
    "     \n",
    "        def scatter_nd(ind,update,shape):\n",
    "            scatter = torch.zeros(shape)\n",
    "            for i in range(len(update)):\n",
    "                scatter[ind[i][0].long(),ind[i][1].long()]=update[i]\n",
    "            return scatter\n",
    "        \n",
    "        labels = all_labels\n",
    "        idx = torch.stack([torch.range(0,self.batch_size-1), labels[:, 0]], axis=1)\n",
    "        label_st = scatter_nd(idx, torch.ones(self.batch_size),[self.batch_size, self.maxlen])\n",
    "        print(label_st.shape)\n",
    "        idx = torch.stack([torch.range(0,self.batch_size-1), labels[:, 1] - 1], axis=1)\n",
    "        label_en = scatter_nd(idx, torch.ones(self.batch_size), [self.batch_size, self.maxlen])\n",
    "        inside_t = sequence_mask(labels[:, 1] - labels[:, 0], self.maxlen)\n",
    "        inside_t = self.flipBatch(inside_t, labels[:, 1])\n",
    "        outside = torch.logical_not(inside_t)\n",
    "        inside_t = inside_t.float()\n",
    "        outside = outside.float()\n",
    "        label = torch.stack([label_st, label_en, inside_t, outside], axis=2)\n",
    "\n",
    "        # Eq. (10)\n",
    "        heavy = label[:, :, :2].sum(-1) > 0.9\n",
    "        heavy = heavy * 9 + 1\n",
    "        label = label / torch.sum(label, axis=2, keepdims=True)\n",
    "        \n",
    "        loss = torch.zeros(logits.shape[0],logits.shape[1])\n",
    "        softmax = torch.softmax(logits,dim=-1)\n",
    "        for k in range(logits.shape[2]):\n",
    "            loss += -label[:,:,k]*torch.log(softmax[:,:,k])\n",
    "        loss *= heavy\n",
    "        mask = sequence_mask(torch.tensor(len_r), self.maxlen)\n",
    "        loss = loss[mask]\n",
    "        loss = torch.mean(loss)\n",
    "        #model_params = optimizer.param_groups\n",
    "        #weights = [i for i in model_params if 'bias' not in i.name]\n",
    "        #loss += params.weight_decay * torch.add_n([torch.nn.MSELoss(v) for v in weights])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self,predictions, label, metric_array):\n",
    "        def get_iou(pred,label):\n",
    "            pred_l, pred_r = torch.unbind(pred,1)\n",
    "            for i in range(2, len(pred.shape)):\n",
    "                label = label.unsqueeze(i)\n",
    "            label_l, label_r = torch.unbind(label,1)\n",
    "            inter_l = torch.max(pred_l, label_l)\n",
    "            inter_r = torch.min(pred_r, label_r)\n",
    "            inter = torch.max((inter_r - inter_l).float(), torch.zeros(inter_l.shape))\n",
    "            union = pred_r - pred_l + label_r - label_l - inter\n",
    "            return inter/union\n",
    "        def get_eval_metric(iou,metric_array):\n",
    "            th = np.arange(0.1, 1.0, 0.1)\n",
    "            vals = [(iou > i).float() for i in th]\n",
    "            for index in range(len(th)):\n",
    "                if round(th[index],1) not in metric_array:\n",
    "                    metric_array[round(th[index],1)] = torch.mean(vals[index])\n",
    "                else:\n",
    "                    metric_array[round(th[index],1)] += torch.mean(vals[index])\n",
    "            vals = torch.stack(vals[4:]).T\n",
    "            vals = torch.mean(vals)\n",
    "            return vals\n",
    "        \n",
    "        iou = get_iou(predictions,label)\n",
    "        metrics = get_eval_metric(iou,metric_array)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "class MatchCellWrapper(nn.Module):\n",
    "    def __init__(self, cell, input_size, output_size, reuse=None):\n",
    "        super(MatchCellWrapper, self).__init__()\n",
    "        self._cell = cell\n",
    "        self.features = input_size\n",
    "        self._attn_vec_size = output_size\n",
    "        self.fcq = nn.Linear(self.features,self._attn_vec_size,bias=None)\n",
    "        self.fcp = nn.Linear(2*self._attn_vec_size,self._attn_vec_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fcg = nn.Linear(self._attn_vec_size,1)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.fchq = nn.Linear(self.features,self._attn_vec_size)\n",
    "        self.fcin = nn.Linear(self.features,self._attn_vec_size)\n",
    "\n",
    "        self.fcx = nn.Linear(self._attn_vec_size,self._attn_vec_size*8)\n",
    "        self.fcy = nn.Linear(self._attn_vec_size,self._attn_vec_size*8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._attn_vec_size\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [self._attn_vec_size, self._attn_vec_size]\n",
    "    \n",
    "    def map_fn(self,func,inputs):\n",
    "        outputs = []\n",
    "        length = inputs[0].shape[0]\n",
    "        for i in range(length):\n",
    "            inp_func = []\n",
    "            for x in inputs:\n",
    "                inp_func.append(x[i])\n",
    "            outputs.append(func(inp_func))\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "    def forward(self, _hq, inputs, state, length):\n",
    "        hq = self.fcq(_hq)\n",
    "        _length = length\n",
    "        h,c = state\n",
    "        concat = torch.cat((inputs, h), axis=1)\n",
    "        hp = self.fcp(concat)\n",
    "        hp = torch.unsqueeze(hp, 1)\n",
    "        g = self.tanh(hq+hp)\n",
    "        g = self.fcg(g)\n",
    "        g = torch.squeeze(g, 2)\n",
    "            \n",
    "        def body(x):\n",
    "            alpha = x[0]\n",
    "            hq = x[1]\n",
    "            length = x[2]\n",
    "            alpha = alpha[:length]\n",
    "            hq = hq[:length]\n",
    "            alpha = self.softmax(alpha)\n",
    "            hq = (hq * alpha[:, None]).sum(0)\n",
    "            return hq\n",
    "        \n",
    "        hq = self.map_fn(body,[g, _hq, _length])\n",
    "        gate_hq = self.fchq(inputs)\n",
    "        gate_in = self.fcin(hq)\n",
    "        gate_hq = self.sigmoid(gate_hq)\n",
    "        gate_in = self.sigmoid(gate_in)\n",
    "        hq1 = hq* gate_hq\n",
    "        inputs1 = inputs * gate_in\n",
    "        \n",
    "        def bilinear(x, y, num_outputs=None, k=8):\n",
    "            _, input_dim = x.shape\n",
    "            if num_outputs is None:\n",
    "                num_outputs = input_dim\n",
    "            x = self.fcx(x)\n",
    "            y = self.fcy(y)\n",
    "            x = torch.reshape(x, [-1, num_outputs, k])\n",
    "            y = torch.reshape(y, [-1, num_outputs, k])\n",
    "            bi = (x * y).sum(-1)\n",
    "            return bi        \n",
    "        \n",
    "        inputs = bilinear(inputs1, hq1)\n",
    "        h,c = self._cell(inputs1, (h,c))        \n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn(args)\n",
    "model.load_state_dict(torch.load(\"modelul900.pt\"))\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=args.learning_rate,weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-299-0d1092cc33e7>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label =[torch.tensor(item[-1]) for item in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-344-90dd541a4fe0>:282: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = self.softmax(alpha)\n",
      "<ipython-input-344-90dd541a4fe0>:122: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  score = self.flipBatch(score, length - torch.range(0,it-1).int())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "tensor(0.2419) {0.1: tensor(0.6878), 0.2: tensor(0.6002), 0.3: tensor(0.5224), 0.4: tensor(0.4282), 0.5: tensor(0.3664), 0.6: tensor(0.3527), 0.7: tensor(0.2589), 0.8: tensor(0.1798), 0.9: tensor(0.0516)}\n"
     ]
    }
   ],
   "source": [
    "    metrics = 0\n",
    "    c = 0\n",
    "    loss = 0\n",
    "    #for epoch in range(100):\n",
    "    metric_array = dict()\n",
    "    for (i,item) in enumerate(data):\n",
    "            print(i)\n",
    "            query,len_q,ref,len_r,labels = item\n",
    "            features = [query,len_q,ref,len_r]\n",
    "            with torch.no_grad():\n",
    "                out = model(features,labels)\n",
    "            predictions = model.predict(out,len_r)\n",
    "            metrics += model.evaluate(predictions,labels,metric_array)\n",
    "            #print(metric_array)\n",
    "            c +=1.0\n",
    "            #loss = model.get_loss(out,labels)\n",
    "            #print (loss)\n",
    "            #optimizer.zero_grad()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "    metrics = metrics/c\n",
    "    for keys in metric_array:\n",
    "        metric_array[keys] /= c \n",
    "    print(metrics, metric_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24188262\n"
     ]
    }
   ],
   "source": [
    "print(metrics.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in metric_array:\n",
    "    metric_array[j] = float(metric_array[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24188262224197388"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.numpy>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU/mean =  0.24188262 \n",
      " {0.1: 0.687785804271698, 0.2: 0.6002048254013062, 0.3: 0.5223894715309143, 0.4: 0.4281631112098694, 0.5: 0.3664014935493469, 0.6: 0.3527296185493469, 0.7: 0.258931964635849, 0.8: 0.17978277802467346, 0.9: 0.05156726390123367}\n"
     ]
    }
   ],
   "source": [
    "print(\"IOU/mean = \",metrics.numpy(),\"\\n\",metric_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = torch.tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
    "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
    "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
    "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
    "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
    "        1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
